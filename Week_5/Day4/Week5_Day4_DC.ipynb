{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6VvbLJQou+PC+s9pGpF7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierrot73/GenAIBootCamp/blob/Bootcamp/Week5_Day4_DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy torch scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Importation de la bibliothèque pandas pour la manipulation des données\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Lecture du fichier csv\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stock_market_dataset.csv')\n",
        "df.head()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Vérifier s'il y a des valeurs manquantes dans tout le DataFrame\n",
        "if df.isnull().values.any():\n",
        "    print(\"Il y a des valeurs manquantes.\")\n",
        "else:\n",
        "    print(\"Il n'y a pas de valeurs manquantes.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Si colonnes catégorielles, afficher leurs valeurs uniques\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "if len(cat_cols) > 0:\n",
        "    for col in cat_cols:\n",
        "        print(f\"\\n{col} :\\n{df[col].value_counts()}\")\n",
        "else:\n",
        "    print(\"Il n'y a pas de valeurs catégorielles\")\n",
        "\n",
        "\n",
        "#Test Statistics with skimpy\n",
        "from skimpy import skim\n",
        "skim(df)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. Supprimer les colonnes inutiles\n",
        "# Selon votre description, les colonnes \"date\", \"symbol\" peuvent être conservées ou supprimées\n",
        "# Si elles ne sont pas nécessaires pour la modélisation, on peut les supprimer\n",
        "df = df.drop(['date', 'symbol'], axis=1)\n",
        "\n",
        "# 2. Créer la colonne 'target' : le prix de clôture du jour suivant\n",
        "# On décale la colonne 'close' d'une ligne vers le haut\n",
        "df['target'] = df['close'].shift(-1)\n",
        "\n",
        "# 3. Supprimer la dernière ligne qui aura un target NaN après décalage\n",
        "df = df.dropna()\n",
        "\n",
        "# 4. Normaliser les données\n",
        "scaler = MinMaxScaler()\n",
        "# On ajuste et transforme toutes les colonnes sauf 'target' si besoin\n",
        "# Mais généralement, on normalise toutes les features, y compris 'close' et autres\n",
        "# Si vous souhaitez normaliser toutes les colonnes :\n",
        "features = df.columns\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=features)\n",
        "\n",
        "# 5. Vérification\n",
        "print(df_scaled.head())\n",
        "\n",
        "# À partir de là, vous pouvez convertir votre DataFrame en tableau numpy pour le modèle LSTM\n",
        "# par exemple :\n",
        "import numpy as np\n",
        "\n",
        "# Convertir en numpy array\n",
        "data = df_scaled.values\n",
        "\n",
        "# Si vous souhaitez créer des séquences pour LSTM :\n",
        "def create_sequences(data, seq_length=50):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length, :-1])  # toutes les features sauf target\n",
        "        y.append(data[i+seq_length, -1])    # target (prix de clôture du lendemain)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Exemple avec une longueur de séquence de 50\n",
        "X, y = create_sequences(data, seq_length=50)\n",
        "\n",
        "# Vérification des dimensions\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# X et y sont déjà préparés\n",
        "# X : numpy array de séquences (nb_samples, sequence_length, nb_features)\n",
        "# y : numpy array des cibles (nb_samples,)\n",
        "\n",
        "# Convertir en tensors PyTorch\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Définir la classe Dataset personnalisée\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Créer l'objet Dataset\n",
        "dataset = StockDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Définir la taille des splits\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)       # 70% pour l'entraînement\n",
        "val_size = int(0.15 * total_size)        # 15% pour la validation\n",
        "test_size = total_size - train_size - val_size  # Reste pour le test\n",
        "\n",
        "# Split le dataset en plusieurs\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Créer les DataLoaders\n",
        "batch_size = 64  # par exemple\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Vérification\n",
        "for batch_X, batch_y in train_loader:\n",
        "    print(\"Batch X shape:\", batch_X.shape)  # (batch_size, seq_length, nb_features)\n",
        "    print(\"Batch y shape:\", batch_y.shape)  # (batch_size,)\n",
        "    break  # Affiche le premier batch pour vérification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_GRU_Model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
        "        super(LSTM_GRU_Model, self).__init__()\n",
        "\n",
        "        # Première couche LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,     # Nombre de features en entrée\n",
        "            hidden_size=hidden_dim,   # Nombre de neurones dans la couche LSTM\n",
        "            num_layers=num_layers,    # Nombre de couches LSTM empilées\n",
        "            batch_first=True,         # Format des données : (batch, seq_len, features)\n",
        "            dropout=dropout if num_layers > 1 else 0  # Dropout entre couches si >1 couche\n",
        "        )\n",
        "\n",
        "        # Couche GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            dropout=0  # Pas de dropout ici, vous pouvez l'ajouter si besoin\n",
        "        )\n",
        "\n",
        "        # Dropout après les couches RNN\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Couche Fully Connected (Dense)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # 1 sortie pour la prédiction continue\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passage dans la couche LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # lstm_out : (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # Passer la dernière sortie de la séquence dans la GRU\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "        # gru_out : (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # Prendre la dernière sortie de la séquence (pour la prédiction finale)\n",
        "        final_hidden_state = gru_out[:, -1, :]  # (batch, hidden_dim)\n",
        "\n",
        "        # Appliquer Dropout\n",
        "        out = self.dropout(final_hidden_state)\n",
        "\n",
        "        # Passer dans la couche Dense\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out.squeeze()  # Retourner un vecteur (batch,)\n",
        "\n",
        "#nb_features correspond au nombre de colonnes features (sans target donc 7).\n",
        "model = LSTM_GRU_Model(input_dim=7, hidden_dim=64, num_layers=2, dropout=0.3)\n",
        "print(model)\n",
        "print(LSTM_GRU_Model)\n",
        "print((model.lstm))\n",
        "print((model.gru))\n",
        "print((model.dropout))\n",
        "print((model.fc))\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Le modèle s'appelle 'model'\n",
        "# et que vous avez déjà défini vos DataLoaders : train_loader, val_loader\n",
        "\n",
        "# 1. Définir la fonction de perte (Loss Function)\n",
        "criterion = nn.MSELoss()  # Pour une tâche de régression\n",
        "\n",
        "# 2. Définir l'optimiseur\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3. Fonction pour entraîner une époque\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in loader:\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Réinitialiser les gradients\n",
        "        outputs = model(batch_X)  # Forward pass\n",
        "        loss = criterion(outputs, batch_y)  # Calcul de la perte\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Mise à jour des paramètres\n",
        "\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# 4. Fonction pour valider une époque\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            total_loss += loss.item() * batch_X.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# 5. Boucle d’entraînement principale\n",
        "num_epochs = 50  # Nombre d’époques\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - \"\n",
        "          f\"Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "import joblib\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_X, test_y in test_loader:\n",
        "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
        "        preds = model(test_X)\n",
        "        all_preds.append(preds.view(-1).cpu())       # Correction ici\n",
        "        all_targets.append(test_y.view(-1).cpu())    # Et ici\n",
        "\n",
        "# Concaténer les prédictions et les cibles\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "# Calcul du R²\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "print(f\"📈 Score R² sur le jeu de test : {r2:.4f}\")\n",
        "\n",
        "# Sauvegarde du scaler\n",
        "joblib.dump(scaler, \"minmax_scaler.pkl\")\n",
        "print(\"✅ Scaler sauvegardé sous minmax_scaler.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fp6gC3rPLEnj",
        "outputId": "63b72a22-7b1e-4f18-f4da-3a59a32ba417"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Il n'y a pas de valeurs manquantes.\n",
            "\n",
            "date :\n",
            "date\n",
            "5-4-2018      1\n",
            "12-27-2021    1\n",
            "12-26-2021    1\n",
            "12-25-2021    1\n",
            "12-24-2021    1\n",
            "             ..\n",
            "12-17-2021    1\n",
            "12-18-2021    1\n",
            "12-19-2021    1\n",
            "12-20-2021    1\n",
            "12-21-2021    1\n",
            "Name: count, Length: 1334, dtype: int64\n",
            "\n",
            "symbol :\n",
            "symbol\n",
            "XRP-USDT    1334\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
              "│ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                          │\n",
              "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
              "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                                          │\n",
              "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
              "│ │ Number of rows    │ 1334   │ │ float64     │ 7     │                                                          │\n",
              "│ │ Number of columns │ 9      │ │ string      │ 2     │                                                          │\n",
              "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
              "│ \u001b[3m                                                    number                                                    \u001b[0m  │\n",
              "│ ┏━━━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓  │\n",
              "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp50     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist  \u001b[0m\u001b[1m \u001b[0m┃  │\n",
              "│ ┡━━━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩  │\n",
              "│ │ \u001b[38;5;141munix    \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m15830000\u001b[0m │ \u001b[36m33280000\u001b[0m │ \u001b[36m15250000\u001b[0m │ \u001b[36m15540000\u001b[0m │ \u001b[36m15830000\u001b[0m │ \u001b[36m16120000\u001b[0m │ \u001b[36m16410000\u001b[0m │ \u001b[32m██████\u001b[0m │  │\n",
              "│ │          │    │      │ \u001b[36m   00000\u001b[0m │ \u001b[36m     000\u001b[0m │ \u001b[36m   00000\u001b[0m │ \u001b[36m   00000\u001b[0m │ \u001b[36m   00000\u001b[0m │ \u001b[36m   00000\u001b[0m │ \u001b[36m   00000\u001b[0m │        │  │\n",
              "│ │ \u001b[38;5;141mopen    \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m  0.4733\u001b[0m │ \u001b[36m  0.3199\u001b[0m │ \u001b[36m  0.1354\u001b[0m │ \u001b[36m  0.2563\u001b[0m │ \u001b[36m   0.326\u001b[0m │ \u001b[36m  0.5652\u001b[0m │ \u001b[36m   1.834\u001b[0m │ \u001b[32m █▃▁▁ \u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mhigh    \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m  0.4956\u001b[0m │ \u001b[36m  0.3412\u001b[0m │ \u001b[36m  0.1494\u001b[0m │ \u001b[36m  0.2636\u001b[0m │ \u001b[36m  0.3376\u001b[0m │ \u001b[36m  0.5999\u001b[0m │ \u001b[36m   1.967\u001b[0m │ \u001b[32m █▃▁▁ \u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mlow     \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m  0.4501\u001b[0m │ \u001b[36m  0.2975\u001b[0m │ \u001b[36m  0.1013\u001b[0m │ \u001b[36m    0.25\u001b[0m │ \u001b[36m  0.3155\u001b[0m │ \u001b[36m  0.5289\u001b[0m │ \u001b[36m   1.652\u001b[0m │ \u001b[32m █▃▁▁ \u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mclose   \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m  0.4736\u001b[0m │ \u001b[36m  0.3201\u001b[0m │ \u001b[36m  0.1355\u001b[0m │ \u001b[36m  0.2563\u001b[0m │ \u001b[36m   0.326\u001b[0m │ \u001b[36m  0.5662\u001b[0m │ \u001b[36m   1.835\u001b[0m │ \u001b[32m █▃▁▁ \u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mVolume  \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m35670000\u001b[0m │ \u001b[36m59150000\u001b[0m │ \u001b[36m 2385000\u001b[0m │ \u001b[36m64490000\u001b[0m │ \u001b[36m14810000\u001b[0m │ \u001b[36m37800000\u001b[0m │ \u001b[36m86080000\u001b[0m │ \u001b[32m  █   \u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mXRP     \u001b[0m │    │      │ \u001b[36m       0\u001b[0m │ \u001b[36m       0\u001b[0m │          │          │ \u001b[36m       0\u001b[0m │ \u001b[36m       0\u001b[0m │ \u001b[36m      00\u001b[0m │        │  │\n",
              "│ │ \u001b[38;5;141mVolume  \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m23560000\u001b[0m │ \u001b[36m48410000\u001b[0m │ \u001b[36m 2198000\u001b[0m │ \u001b[36m18940000\u001b[0m │ \u001b[36m41360000\u001b[0m │ \u001b[36m25190000\u001b[0m │ \u001b[36m45900000\u001b[0m │ \u001b[32m  █   \u001b[0m │  │\n",
              "│ │ \u001b[38;5;141mUSDT    \u001b[0m │    │      │ \u001b[36m       0\u001b[0m │ \u001b[36m       0\u001b[0m │          │          │          │ \u001b[36m       0\u001b[0m │ \u001b[36m      00\u001b[0m │        │  │\n",
              "│ └──────────┴────┴──────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴────────┘  │\n",
              "│ \u001b[3m                                                    string                                                    \u001b[0m  │\n",
              "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓  │\n",
              "│ ┃\u001b[1m        \u001b[0m┃\u001b[1m    \u001b[0m┃\u001b[1m      \u001b[0m┃\u001b[1m          \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m          \u001b[0m┃\u001b[1m          \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mchars per  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords per  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m             \u001b[0m┃  │\n",
              "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mshortest\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlongest   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmin     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmax     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mtotal words\u001b[0m\u001b[1m \u001b[0m┃  │\n",
              "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩  │\n",
              "│ │ \u001b[38;5;141mdate  \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[38;5;141m9-9-2021\u001b[0m │ \u001b[38;5;141m12-27-2021\u001b[0m │ \u001b[38;5;141m1-1-2019\u001b[0m │ \u001b[38;5;141m9-9-2021\u001b[0m │ \u001b[36m       8.98\u001b[0m │ \u001b[36m          1\u001b[0m │ \u001b[36m       1334\u001b[0m │  │\n",
              "│ │ \u001b[38;5;141msymbol\u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[38;5;141mXRP-USDT\u001b[0m │ \u001b[38;5;141mXRP-USDT  \u001b[0m │ \u001b[38;5;141mXRP-USDT\u001b[0m │ \u001b[38;5;141mXRP-USDT\u001b[0m │ \u001b[36m          8\u001b[0m │ \u001b[36m          1\u001b[0m │ \u001b[36m       1334\u001b[0m │  │\n",
              "│ └────────┴────┴──────┴──────────┴────────────┴──────────┴──────────┴─────────────┴─────────────┴─────────────┘  │\n",
              "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
              "│ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                                          │\n",
              "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
              "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                                          │\n",
              "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
              "│ │ Number of rows    │ 1334   │ │ float64     │ 7     │                                                          │\n",
              "│ │ Number of columns │ 9      │ │ string      │ 2     │                                                          │\n",
              "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
              "│ <span style=\"font-style: italic\">                                                    number                                                    </span>  │\n",
              "│ ┏━━━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┓  │\n",
              "│ ┃<span style=\"font-weight: bold\"> column   </span>┃<span style=\"font-weight: bold\"> NA </span>┃<span style=\"font-weight: bold\"> NA % </span>┃<span style=\"font-weight: bold\"> mean     </span>┃<span style=\"font-weight: bold\"> sd       </span>┃<span style=\"font-weight: bold\"> p0       </span>┃<span style=\"font-weight: bold\"> p25      </span>┃<span style=\"font-weight: bold\"> p50      </span>┃<span style=\"font-weight: bold\"> p75      </span>┃<span style=\"font-weight: bold\"> p100     </span>┃<span style=\"font-weight: bold\"> hist   </span>┃  │\n",
              "│ ┡━━━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━┩  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">unix    </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">15830000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">33280000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">15250000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">15540000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">15830000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">16120000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">16410000</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">██████</span> │  │\n",
              "│ │          │    │      │ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">     000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> │        │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">open    </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4733</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3199</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1354</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2563</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0.326</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5652</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   1.834</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> █▃▁▁ </span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">high    </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4956</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3412</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1494</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2636</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3376</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5999</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   1.967</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> █▃▁▁ </span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">low     </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4501</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2975</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1013</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0.25</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3155</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5289</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   1.652</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> █▃▁▁ </span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">close   </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4736</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3201</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1355</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2563</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0.326</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5662</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   1.835</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> █▃▁▁ </span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Volume  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">35670000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">59150000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 2385000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">64490000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">14810000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">37800000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">86080000</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">  █   </span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP     </span> │    │      │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │          │          │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      00</span> │        │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Volume  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">23560000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">48410000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 2198000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">18940000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">41360000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">25190000</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">45900000</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">  █   </span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">USDT    </span> │    │      │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │          │          │          │ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">      00</span> │        │  │\n",
              "│ └──────────┴────┴──────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴──────────┴────────┘  │\n",
              "│ <span style=\"font-style: italic\">                                                    string                                                    </span>  │\n",
              "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓  │\n",
              "│ ┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">    </span>┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">          </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">          </span>┃<span style=\"font-weight: bold\">          </span>┃<span style=\"font-weight: bold\"> chars per   </span>┃<span style=\"font-weight: bold\"> words per   </span>┃<span style=\"font-weight: bold\">             </span>┃  │\n",
              "│ ┃<span style=\"font-weight: bold\"> column </span>┃<span style=\"font-weight: bold\"> NA </span>┃<span style=\"font-weight: bold\"> NA % </span>┃<span style=\"font-weight: bold\"> shortest </span>┃<span style=\"font-weight: bold\"> longest    </span>┃<span style=\"font-weight: bold\"> min      </span>┃<span style=\"font-weight: bold\"> max      </span>┃<span style=\"font-weight: bold\"> row         </span>┃<span style=\"font-weight: bold\"> row         </span>┃<span style=\"font-weight: bold\"> total words </span>┃  │\n",
              "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">date  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">9-9-2021</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">12-27-2021</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">1-1-2019</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">9-9-2021</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       8.98</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">          1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       1334</span> │  │\n",
              "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">symbol</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">          8</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">          1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">       1334</span> │  │\n",
              "│ └────────┴────┴──────┴──────────┴────────────┴──────────┴──────────┴─────────────┴─────────────┴─────────────┘  │\n",
              "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       unix      open      high       low     close  Volume XRP  Volume USDT  \\\n",
            "0  1.000000  0.461933  0.426033  0.527812  0.463227    0.000000     0.000000   \n",
            "1  0.999305  0.464995  0.431370  0.518270  0.461696    0.018714     0.032203   \n",
            "2  0.998523  0.456870  0.432251  0.513693  0.464757    0.028781     0.049704   \n",
            "3  0.997741  0.505558  0.466143  0.512597  0.456694    0.065635     0.116724   \n",
            "4  0.997046  0.481832  0.477202  0.538900  0.505305    0.055433     0.102607   \n",
            "\n",
            "     target  \n",
            "0  0.461696  \n",
            "1  0.464757  \n",
            "2  0.456694  \n",
            "3  0.505305  \n",
            "4  0.481529  \n",
            "X shape: (1283, 50, 7)\n",
            "y shape: (1283,)\n",
            "Batch X shape: torch.Size([64, 50, 7])\n",
            "Batch y shape: torch.Size([64])\n",
            "LSTM_GRU_Model(\n",
            "  (lstm): LSTM(7, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (gru): GRU(64, 64, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "<class '__main__.LSTM_GRU_Model'>\n",
            "LSTM(7, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
            "GRU(64, 64, batch_first=True)\n",
            "Dropout(p=0.3, inplace=False)\n",
            "Linear(in_features=64, out_features=1, bias=True)\n",
            "Epoch 1/50 - Training Loss: 0.0584 - Validation Loss: 0.0288\n",
            "Epoch 2/50 - Training Loss: 0.0243 - Validation Loss: 0.0119\n",
            "Epoch 3/50 - Training Loss: 0.0099 - Validation Loss: 0.0072\n",
            "Epoch 4/50 - Training Loss: 0.0067 - Validation Loss: 0.0049\n",
            "Epoch 5/50 - Training Loss: 0.0054 - Validation Loss: 0.0037\n",
            "Epoch 6/50 - Training Loss: 0.0047 - Validation Loss: 0.0036\n",
            "Epoch 7/50 - Training Loss: 0.0048 - Validation Loss: 0.0027\n",
            "Epoch 8/50 - Training Loss: 0.0038 - Validation Loss: 0.0029\n",
            "Epoch 9/50 - Training Loss: 0.0045 - Validation Loss: 0.0039\n",
            "Epoch 10/50 - Training Loss: 0.0049 - Validation Loss: 0.0023\n",
            "Epoch 11/50 - Training Loss: 0.0042 - Validation Loss: 0.0023\n",
            "Epoch 12/50 - Training Loss: 0.0040 - Validation Loss: 0.0030\n",
            "Epoch 13/50 - Training Loss: 0.0034 - Validation Loss: 0.0021\n",
            "Epoch 14/50 - Training Loss: 0.0038 - Validation Loss: 0.0021\n",
            "Epoch 15/50 - Training Loss: 0.0030 - Validation Loss: 0.0018\n",
            "Epoch 16/50 - Training Loss: 0.0032 - Validation Loss: 0.0040\n",
            "Epoch 17/50 - Training Loss: 0.0067 - Validation Loss: 0.0029\n",
            "Epoch 18/50 - Training Loss: 0.0050 - Validation Loss: 0.0018\n",
            "Epoch 19/50 - Training Loss: 0.0038 - Validation Loss: 0.0021\n",
            "Epoch 20/50 - Training Loss: 0.0050 - Validation Loss: 0.0019\n",
            "Epoch 21/50 - Training Loss: 0.0033 - Validation Loss: 0.0019\n",
            "Epoch 22/50 - Training Loss: 0.0027 - Validation Loss: 0.0014\n",
            "Epoch 23/50 - Training Loss: 0.0024 - Validation Loss: 0.0013\n",
            "Epoch 24/50 - Training Loss: 0.0028 - Validation Loss: 0.0017\n",
            "Epoch 25/50 - Training Loss: 0.0054 - Validation Loss: 0.0025\n",
            "Epoch 26/50 - Training Loss: 0.0032 - Validation Loss: 0.0016\n",
            "Epoch 27/50 - Training Loss: 0.0030 - Validation Loss: 0.0014\n",
            "Epoch 28/50 - Training Loss: 0.0028 - Validation Loss: 0.0016\n",
            "Epoch 29/50 - Training Loss: 0.0025 - Validation Loss: 0.0019\n",
            "Epoch 30/50 - Training Loss: 0.0029 - Validation Loss: 0.0012\n",
            "Epoch 31/50 - Training Loss: 0.0026 - Validation Loss: 0.0020\n",
            "Epoch 32/50 - Training Loss: 0.0025 - Validation Loss: 0.0013\n",
            "Epoch 33/50 - Training Loss: 0.0028 - Validation Loss: 0.0011\n",
            "Epoch 34/50 - Training Loss: 0.0027 - Validation Loss: 0.0012\n",
            "Epoch 35/50 - Training Loss: 0.0027 - Validation Loss: 0.0027\n",
            "Epoch 36/50 - Training Loss: 0.0059 - Validation Loss: 0.0014\n",
            "Epoch 37/50 - Training Loss: 0.0030 - Validation Loss: 0.0013\n",
            "Epoch 38/50 - Training Loss: 0.0027 - Validation Loss: 0.0012\n",
            "Epoch 39/50 - Training Loss: 0.0024 - Validation Loss: 0.0011\n",
            "Epoch 40/50 - Training Loss: 0.0024 - Validation Loss: 0.0011\n",
            "Epoch 41/50 - Training Loss: 0.0021 - Validation Loss: 0.0011\n",
            "Epoch 42/50 - Training Loss: 0.0020 - Validation Loss: 0.0012\n",
            "Epoch 43/50 - Training Loss: 0.0039 - Validation Loss: 0.0013\n",
            "Epoch 44/50 - Training Loss: 0.0028 - Validation Loss: 0.0016\n",
            "Epoch 45/50 - Training Loss: 0.0026 - Validation Loss: 0.0014\n",
            "Epoch 46/50 - Training Loss: 0.0026 - Validation Loss: 0.0015\n",
            "Epoch 47/50 - Training Loss: 0.0025 - Validation Loss: 0.0012\n",
            "Epoch 48/50 - Training Loss: 0.0025 - Validation Loss: 0.0011\n",
            "Epoch 49/50 - Training Loss: 0.0021 - Validation Loss: 0.0010\n",
            "Epoch 50/50 - Training Loss: 0.0021 - Validation Loss: 0.0010\n",
            "📈 Score R² sur le jeu de test : 0.9678\n",
            "✅ Scaler sauvegardé sous minmax_scaler.pkl\n"
          ]
        }
      ]
    }
  ]
}
