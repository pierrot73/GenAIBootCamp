{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6VvbLJQou+PC+s9pGpF7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierrot73/GenAIBootCamp/blob/Bootcamp/Week5_Day4_DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy torch scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Importation de la bibliothÃ¨que pandas pour la manipulation des donnÃ©es\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Lecture du fichier csv\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/stock_market_dataset.csv')\n",
        "df.head()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# VÃ©rifier s'il y a des valeurs manquantes dans tout le DataFrame\n",
        "if df.isnull().values.any():\n",
        "    print(\"Il y a des valeurs manquantes.\")\n",
        "else:\n",
        "    print(\"Il n'y a pas de valeurs manquantes.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Si colonnes catÃ©gorielles, afficher leurs valeurs uniques\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "if len(cat_cols) > 0:\n",
        "    for col in cat_cols:\n",
        "        print(f\"\\n{col} :\\n{df[col].value_counts()}\")\n",
        "else:\n",
        "    print(\"Il n'y a pas de valeurs catÃ©gorielles\")\n",
        "\n",
        "\n",
        "#Test Statistics with skimpy\n",
        "from skimpy import skim\n",
        "skim(df)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. Supprimer les colonnes inutiles\n",
        "# Selon votre description, les colonnes \"date\", \"symbol\" peuvent Ãªtre conservÃ©es ou supprimÃ©es\n",
        "# Si elles ne sont pas nÃ©cessaires pour la modÃ©lisation, on peut les supprimer\n",
        "df = df.drop(['date', 'symbol'], axis=1)\n",
        "\n",
        "# 2. CrÃ©er la colonne 'target' : le prix de clÃ´ture du jour suivant\n",
        "# On dÃ©cale la colonne 'close' d'une ligne vers le haut\n",
        "df['target'] = df['close'].shift(-1)\n",
        "\n",
        "# 3. Supprimer la derniÃ¨re ligne qui aura un target NaN aprÃ¨s dÃ©calage\n",
        "df = df.dropna()\n",
        "\n",
        "# 4. Normaliser les donnÃ©es\n",
        "scaler = MinMaxScaler()\n",
        "# On ajuste et transforme toutes les colonnes sauf 'target' si besoin\n",
        "# Mais gÃ©nÃ©ralement, on normalise toutes les features, y compris 'close' et autres\n",
        "# Si vous souhaitez normaliser toutes les colonnes :\n",
        "features = df.columns\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=features)\n",
        "\n",
        "# 5. VÃ©rification\n",
        "print(df_scaled.head())\n",
        "\n",
        "# Ã€ partir de lÃ , vous pouvez convertir votre DataFrame en tableau numpy pour le modÃ¨le LSTM\n",
        "# par exemple :\n",
        "import numpy as np\n",
        "\n",
        "# Convertir en numpy array\n",
        "data = df_scaled.values\n",
        "\n",
        "# Si vous souhaitez crÃ©er des sÃ©quences pour LSTM :\n",
        "def create_sequences(data, seq_length=50):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length, :-1])  # toutes les features sauf target\n",
        "        y.append(data[i+seq_length, -1])    # target (prix de clÃ´ture du lendemain)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Exemple avec une longueur de sÃ©quence de 50\n",
        "X, y = create_sequences(data, seq_length=50)\n",
        "\n",
        "# VÃ©rification des dimensions\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# X et y sont dÃ©jÃ  prÃ©parÃ©s\n",
        "# X : numpy array de sÃ©quences (nb_samples, sequence_length, nb_features)\n",
        "# y : numpy array des cibles (nb_samples,)\n",
        "\n",
        "# Convertir en tensors PyTorch\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# DÃ©finir la classe Dataset personnalisÃ©e\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# CrÃ©er l'objet Dataset\n",
        "dataset = StockDataset(X_tensor, y_tensor)\n",
        "\n",
        "# DÃ©finir la taille des splits\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)       # 70% pour l'entraÃ®nement\n",
        "val_size = int(0.15 * total_size)        # 15% pour la validation\n",
        "test_size = total_size - train_size - val_size  # Reste pour le test\n",
        "\n",
        "# Split le dataset en plusieurs\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# CrÃ©er les DataLoaders\n",
        "batch_size = 64  # par exemple\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# VÃ©rification\n",
        "for batch_X, batch_y in train_loader:\n",
        "    print(\"Batch X shape:\", batch_X.shape)  # (batch_size, seq_length, nb_features)\n",
        "    print(\"Batch y shape:\", batch_y.shape)  # (batch_size,)\n",
        "    break  # Affiche le premier batch pour vÃ©rification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_GRU_Model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
        "        super(LSTM_GRU_Model, self).__init__()\n",
        "\n",
        "        # PremiÃ¨re couche LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,     # Nombre de features en entrÃ©e\n",
        "            hidden_size=hidden_dim,   # Nombre de neurones dans la couche LSTM\n",
        "            num_layers=num_layers,    # Nombre de couches LSTM empilÃ©es\n",
        "            batch_first=True,         # Format des donnÃ©es : (batch, seq_len, features)\n",
        "            dropout=dropout if num_layers > 1 else 0  # Dropout entre couches si >1 couche\n",
        "        )\n",
        "\n",
        "        # Couche GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            dropout=0  # Pas de dropout ici, vous pouvez l'ajouter si besoin\n",
        "        )\n",
        "\n",
        "        # Dropout aprÃ¨s les couches RNN\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Couche Fully Connected (Dense)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # 1 sortie pour la prÃ©diction continue\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passage dans la couche LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # lstm_out : (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # Passer la derniÃ¨re sortie de la sÃ©quence dans la GRU\n",
        "        gru_out, _ = self.gru(lstm_out)\n",
        "        # gru_out : (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # Prendre la derniÃ¨re sortie de la sÃ©quence (pour la prÃ©diction finale)\n",
        "        final_hidden_state = gru_out[:, -1, :]  # (batch, hidden_dim)\n",
        "\n",
        "        # Appliquer Dropout\n",
        "        out = self.dropout(final_hidden_state)\n",
        "\n",
        "        # Passer dans la couche Dense\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out.squeeze()  # Retourner un vecteur (batch,)\n",
        "\n",
        "#nb_features correspond au nombre de colonnes features (sans target donc 7).\n",
        "model = LSTM_GRU_Model(input_dim=7, hidden_dim=64, num_layers=2, dropout=0.3)\n",
        "print(model)\n",
        "print(LSTM_GRU_Model)\n",
        "print((model.lstm))\n",
        "print((model.gru))\n",
        "print((model.dropout))\n",
        "print((model.fc))\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Le modÃ¨le s'appelle 'model'\n",
        "# et que vous avez dÃ©jÃ  dÃ©fini vos DataLoaders : train_loader, val_loader\n",
        "\n",
        "# 1. DÃ©finir la fonction de perte (Loss Function)\n",
        "criterion = nn.MSELoss()  # Pour une tÃ¢che de rÃ©gression\n",
        "\n",
        "# 2. DÃ©finir l'optimiseur\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3. Fonction pour entraÃ®ner une Ã©poque\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in loader:\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # RÃ©initialiser les gradients\n",
        "        outputs = model(batch_X)  # Forward pass\n",
        "        loss = criterion(outputs, batch_y)  # Calcul de la perte\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Mise Ã  jour des paramÃ¨tres\n",
        "\n",
        "        total_loss += loss.item() * batch_X.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# 4. Fonction pour valider une Ã©poque\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            total_loss += loss.item() * batch_X.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# 5. Boucle dâ€™entraÃ®nement principale\n",
        "num_epochs = 50  # Nombre dâ€™Ã©poques\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - \"\n",
        "          f\"Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "import joblib\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_X, test_y in test_loader:\n",
        "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
        "        preds = model(test_X)\n",
        "        all_preds.append(preds.view(-1).cpu())       # Correction ici\n",
        "        all_targets.append(test_y.view(-1).cpu())    # Et ici\n",
        "\n",
        "# ConcatÃ©ner les prÃ©dictions et les cibles\n",
        "all_preds = torch.cat(all_preds).numpy()\n",
        "all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "# Calcul du RÂ²\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "print(f\"ğŸ“ˆ Score RÂ² sur le jeu de test : {r2:.4f}\")\n",
        "\n",
        "# Sauvegarde du scaler\n",
        "joblib.dump(scaler, \"minmax_scaler.pkl\")\n",
        "print(\"âœ… Scaler sauvegardÃ© sous minmax_scaler.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fp6gC3rPLEnj",
        "outputId": "63b72a22-7b1e-4f18-f4da-3a59a32ba417"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Il n'y a pas de valeurs manquantes.\n",
            "\n",
            "date :\n",
            "date\n",
            "5-4-2018      1\n",
            "12-27-2021    1\n",
            "12-26-2021    1\n",
            "12-25-2021    1\n",
            "12-24-2021    1\n",
            "             ..\n",
            "12-17-2021    1\n",
            "12-18-2021    1\n",
            "12-19-2021    1\n",
            "12-20-2021    1\n",
            "12-21-2021    1\n",
            "Name: count, Length: 1334, dtype: int64\n",
            "\n",
            "symbol :\n",
            "symbol\n",
            "XRP-USDT    1334\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ skimpy summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                          â”‚\n",
              "â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“                                                          â”‚\n",
              "â”‚ â”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ â”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0mâ”ƒ                                                          â”‚\n",
              "â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”© â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©                                                          â”‚\n",
              "â”‚ â”‚ Number of rows    â”‚ 1334   â”‚ â”‚ float64     â”‚ 7     â”‚                                                          â”‚\n",
              "â”‚ â”‚ Number of columns â”‚ 9      â”‚ â”‚ string      â”‚ 2     â”‚                                                          â”‚\n",
              "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚\n",
              "â”‚ \u001b[3m                                                    number                                                    \u001b[0m  â”‚\n",
              "â”‚ â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“  â”‚\n",
              "â”‚ â”ƒ\u001b[1m \u001b[0m\u001b[1mcolumn  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mmean    \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1msd      \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mp0      \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mp25     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mp50     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mp75     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mp100    \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mhist  \u001b[0m\u001b[1m \u001b[0mâ”ƒ  â”‚\n",
              "â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141munix    \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m15830000\u001b[0m â”‚ \u001b[36m33280000\u001b[0m â”‚ \u001b[36m15250000\u001b[0m â”‚ \u001b[36m15540000\u001b[0m â”‚ \u001b[36m15830000\u001b[0m â”‚ \u001b[36m16120000\u001b[0m â”‚ \u001b[36m16410000\u001b[0m â”‚ \u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚          â”‚    â”‚      â”‚ \u001b[36m   00000\u001b[0m â”‚ \u001b[36m     000\u001b[0m â”‚ \u001b[36m   00000\u001b[0m â”‚ \u001b[36m   00000\u001b[0m â”‚ \u001b[36m   00000\u001b[0m â”‚ \u001b[36m   00000\u001b[0m â”‚ \u001b[36m   00000\u001b[0m â”‚        â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mopen    \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m  0.4733\u001b[0m â”‚ \u001b[36m  0.3199\u001b[0m â”‚ \u001b[36m  0.1354\u001b[0m â”‚ \u001b[36m  0.2563\u001b[0m â”‚ \u001b[36m   0.326\u001b[0m â”‚ \u001b[36m  0.5652\u001b[0m â”‚ \u001b[36m   1.834\u001b[0m â”‚ \u001b[32m â–ˆâ–ƒâ–â– \u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mhigh    \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m  0.4956\u001b[0m â”‚ \u001b[36m  0.3412\u001b[0m â”‚ \u001b[36m  0.1494\u001b[0m â”‚ \u001b[36m  0.2636\u001b[0m â”‚ \u001b[36m  0.3376\u001b[0m â”‚ \u001b[36m  0.5999\u001b[0m â”‚ \u001b[36m   1.967\u001b[0m â”‚ \u001b[32m â–ˆâ–ƒâ–â– \u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mlow     \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m  0.4501\u001b[0m â”‚ \u001b[36m  0.2975\u001b[0m â”‚ \u001b[36m  0.1013\u001b[0m â”‚ \u001b[36m    0.25\u001b[0m â”‚ \u001b[36m  0.3155\u001b[0m â”‚ \u001b[36m  0.5289\u001b[0m â”‚ \u001b[36m   1.652\u001b[0m â”‚ \u001b[32m â–ˆâ–ƒâ–â– \u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mclose   \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m  0.4736\u001b[0m â”‚ \u001b[36m  0.3201\u001b[0m â”‚ \u001b[36m  0.1355\u001b[0m â”‚ \u001b[36m  0.2563\u001b[0m â”‚ \u001b[36m   0.326\u001b[0m â”‚ \u001b[36m  0.5662\u001b[0m â”‚ \u001b[36m   1.835\u001b[0m â”‚ \u001b[32m â–ˆâ–ƒâ–â– \u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mVolume  \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m35670000\u001b[0m â”‚ \u001b[36m59150000\u001b[0m â”‚ \u001b[36m 2385000\u001b[0m â”‚ \u001b[36m64490000\u001b[0m â”‚ \u001b[36m14810000\u001b[0m â”‚ \u001b[36m37800000\u001b[0m â”‚ \u001b[36m86080000\u001b[0m â”‚ \u001b[32m  â–ˆ   \u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mXRP     \u001b[0m â”‚    â”‚      â”‚ \u001b[36m       0\u001b[0m â”‚ \u001b[36m       0\u001b[0m â”‚          â”‚          â”‚ \u001b[36m       0\u001b[0m â”‚ \u001b[36m       0\u001b[0m â”‚ \u001b[36m      00\u001b[0m â”‚        â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mVolume  \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[36m23560000\u001b[0m â”‚ \u001b[36m48410000\u001b[0m â”‚ \u001b[36m 2198000\u001b[0m â”‚ \u001b[36m18940000\u001b[0m â”‚ \u001b[36m41360000\u001b[0m â”‚ \u001b[36m25190000\u001b[0m â”‚ \u001b[36m45900000\u001b[0m â”‚ \u001b[32m  â–ˆ   \u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mUSDT    \u001b[0m â”‚    â”‚      â”‚ \u001b[36m       0\u001b[0m â”‚ \u001b[36m       0\u001b[0m â”‚          â”‚          â”‚          â”‚ \u001b[36m       0\u001b[0m â”‚ \u001b[36m      00\u001b[0m â”‚        â”‚  â”‚\n",
              "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
              "â”‚ \u001b[3m                                                    string                                                    \u001b[0m  â”‚\n",
              "â”‚ â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“  â”‚\n",
              "â”‚ â”ƒ\u001b[1m        \u001b[0mâ”ƒ\u001b[1m    \u001b[0mâ”ƒ\u001b[1m      \u001b[0mâ”ƒ\u001b[1m          \u001b[0mâ”ƒ\u001b[1m            \u001b[0mâ”ƒ\u001b[1m          \u001b[0mâ”ƒ\u001b[1m          \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mchars per  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mwords per  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m             \u001b[0mâ”ƒ  â”‚\n",
              "â”‚ â”ƒ\u001b[1m \u001b[0m\u001b[1mcolumn\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mshortest\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mlongest   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mmin     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mmax     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mrow        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mrow        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mtotal words\u001b[0m\u001b[1m \u001b[0mâ”ƒ  â”‚\n",
              "â”‚ â”¡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141mdate  \u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[38;5;141m9-9-2021\u001b[0m â”‚ \u001b[38;5;141m12-27-2021\u001b[0m â”‚ \u001b[38;5;141m1-1-2019\u001b[0m â”‚ \u001b[38;5;141m9-9-2021\u001b[0m â”‚ \u001b[36m       8.98\u001b[0m â”‚ \u001b[36m          1\u001b[0m â”‚ \u001b[36m       1334\u001b[0m â”‚  â”‚\n",
              "â”‚ â”‚ \u001b[38;5;141msymbol\u001b[0m â”‚ \u001b[36m 0\u001b[0m â”‚ \u001b[36m   0\u001b[0m â”‚ \u001b[38;5;141mXRP-USDT\u001b[0m â”‚ \u001b[38;5;141mXRP-USDT  \u001b[0m â”‚ \u001b[38;5;141mXRP-USDT\u001b[0m â”‚ \u001b[38;5;141mXRP-USDT\u001b[0m â”‚ \u001b[36m          8\u001b[0m â”‚ \u001b[36m          1\u001b[0m â”‚ \u001b[36m       1334\u001b[0m â”‚  â”‚\n",
              "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ End â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ skimpy summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                                          â”‚\n",
              "â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“                                                          â”‚\n",
              "â”‚ â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>â”ƒ â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>â”ƒ<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>â”ƒ                                                          â”‚\n",
              "â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”© â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©                                                          â”‚\n",
              "â”‚ â”‚ Number of rows    â”‚ 1334   â”‚ â”‚ float64     â”‚ 7     â”‚                                                          â”‚\n",
              "â”‚ â”‚ Number of columns â”‚ 9      â”‚ â”‚ string      â”‚ 2     â”‚                                                          â”‚\n",
              "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                                                          â”‚\n",
              "â”‚ <span style=\"font-style: italic\">                                                    number                                                    </span>  â”‚\n",
              "â”‚ â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“  â”‚\n",
              "â”‚ â”ƒ<span style=\"font-weight: bold\"> column   </span>â”ƒ<span style=\"font-weight: bold\"> NA </span>â”ƒ<span style=\"font-weight: bold\"> NA % </span>â”ƒ<span style=\"font-weight: bold\"> mean     </span>â”ƒ<span style=\"font-weight: bold\"> sd       </span>â”ƒ<span style=\"font-weight: bold\"> p0       </span>â”ƒ<span style=\"font-weight: bold\"> p25      </span>â”ƒ<span style=\"font-weight: bold\"> p50      </span>â”ƒ<span style=\"font-weight: bold\"> p75      </span>â”ƒ<span style=\"font-weight: bold\"> p100     </span>â”ƒ<span style=\"font-weight: bold\"> hist   </span>â”ƒ  â”‚\n",
              "â”‚ â”¡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">unix    </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">15830000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">33280000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">15250000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">15540000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">15830000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">16120000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">16410000</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\">â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</span> â”‚  â”‚\n",
              "â”‚ â”‚          â”‚    â”‚      â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">     000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   00000</span> â”‚        â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">open    </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4733</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3199</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1354</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2563</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0.326</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5652</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   1.834</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\"> â–ˆâ–ƒâ–â– </span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">high    </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4956</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3412</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1494</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2636</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3376</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5999</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   1.967</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\"> â–ˆâ–ƒâ–â– </span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">low     </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4501</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2975</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1013</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">    0.25</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3155</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5289</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   1.652</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\"> â–ˆâ–ƒâ–â– </span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">close   </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.4736</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.3201</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.1355</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.2563</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0.326</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">  0.5662</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   1.835</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\"> â–ˆâ–ƒâ–â– </span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Volume  </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">35670000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">59150000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 2385000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">64490000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">14810000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">37800000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">86080000</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\">  â–ˆ   </span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP     </span> â”‚    â”‚      â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚          â”‚          â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">      00</span> â”‚        â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Volume  </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">23560000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">48410000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 2198000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">18940000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">41360000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">25190000</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">45900000</span> â”‚ <span style=\"color: #008000; text-decoration-color: #008000\">  â–ˆ   </span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">USDT    </span> â”‚    â”‚      â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚          â”‚          â”‚          â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">      00</span> â”‚        â”‚  â”‚\n",
              "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
              "â”‚ <span style=\"font-style: italic\">                                                    string                                                    </span>  â”‚\n",
              "â”‚ â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”³â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“  â”‚\n",
              "â”‚ â”ƒ<span style=\"font-weight: bold\">        </span>â”ƒ<span style=\"font-weight: bold\">    </span>â”ƒ<span style=\"font-weight: bold\">      </span>â”ƒ<span style=\"font-weight: bold\">          </span>â”ƒ<span style=\"font-weight: bold\">            </span>â”ƒ<span style=\"font-weight: bold\">          </span>â”ƒ<span style=\"font-weight: bold\">          </span>â”ƒ<span style=\"font-weight: bold\"> chars per   </span>â”ƒ<span style=\"font-weight: bold\"> words per   </span>â”ƒ<span style=\"font-weight: bold\">             </span>â”ƒ  â”‚\n",
              "â”‚ â”ƒ<span style=\"font-weight: bold\"> column </span>â”ƒ<span style=\"font-weight: bold\"> NA </span>â”ƒ<span style=\"font-weight: bold\"> NA % </span>â”ƒ<span style=\"font-weight: bold\"> shortest </span>â”ƒ<span style=\"font-weight: bold\"> longest    </span>â”ƒ<span style=\"font-weight: bold\"> min      </span>â”ƒ<span style=\"font-weight: bold\"> max      </span>â”ƒ<span style=\"font-weight: bold\"> row         </span>â”ƒ<span style=\"font-weight: bold\"> row         </span>â”ƒ<span style=\"font-weight: bold\"> total words </span>â”ƒ  â”‚\n",
              "â”‚ â”¡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â•‡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">date  </span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">9-9-2021</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">12-27-2021</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">1-1-2019</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">9-9-2021</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       8.98</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">          1</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       1334</span> â”‚  â”‚\n",
              "â”‚ â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">symbol</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT  </span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT</span> â”‚ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">XRP-USDT</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">          8</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">          1</span> â”‚ <span style=\"color: #008080; text-decoration-color: #008080\">       1334</span> â”‚  â”‚\n",
              "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ End â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       unix      open      high       low     close  Volume XRP  Volume USDT  \\\n",
            "0  1.000000  0.461933  0.426033  0.527812  0.463227    0.000000     0.000000   \n",
            "1  0.999305  0.464995  0.431370  0.518270  0.461696    0.018714     0.032203   \n",
            "2  0.998523  0.456870  0.432251  0.513693  0.464757    0.028781     0.049704   \n",
            "3  0.997741  0.505558  0.466143  0.512597  0.456694    0.065635     0.116724   \n",
            "4  0.997046  0.481832  0.477202  0.538900  0.505305    0.055433     0.102607   \n",
            "\n",
            "     target  \n",
            "0  0.461696  \n",
            "1  0.464757  \n",
            "2  0.456694  \n",
            "3  0.505305  \n",
            "4  0.481529  \n",
            "X shape: (1283, 50, 7)\n",
            "y shape: (1283,)\n",
            "Batch X shape: torch.Size([64, 50, 7])\n",
            "Batch y shape: torch.Size([64])\n",
            "LSTM_GRU_Model(\n",
            "  (lstm): LSTM(7, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (gru): GRU(64, 64, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "<class '__main__.LSTM_GRU_Model'>\n",
            "LSTM(7, 64, num_layers=2, batch_first=True, dropout=0.3)\n",
            "GRU(64, 64, batch_first=True)\n",
            "Dropout(p=0.3, inplace=False)\n",
            "Linear(in_features=64, out_features=1, bias=True)\n",
            "Epoch 1/50 - Training Loss: 0.0584 - Validation Loss: 0.0288\n",
            "Epoch 2/50 - Training Loss: 0.0243 - Validation Loss: 0.0119\n",
            "Epoch 3/50 - Training Loss: 0.0099 - Validation Loss: 0.0072\n",
            "Epoch 4/50 - Training Loss: 0.0067 - Validation Loss: 0.0049\n",
            "Epoch 5/50 - Training Loss: 0.0054 - Validation Loss: 0.0037\n",
            "Epoch 6/50 - Training Loss: 0.0047 - Validation Loss: 0.0036\n",
            "Epoch 7/50 - Training Loss: 0.0048 - Validation Loss: 0.0027\n",
            "Epoch 8/50 - Training Loss: 0.0038 - Validation Loss: 0.0029\n",
            "Epoch 9/50 - Training Loss: 0.0045 - Validation Loss: 0.0039\n",
            "Epoch 10/50 - Training Loss: 0.0049 - Validation Loss: 0.0023\n",
            "Epoch 11/50 - Training Loss: 0.0042 - Validation Loss: 0.0023\n",
            "Epoch 12/50 - Training Loss: 0.0040 - Validation Loss: 0.0030\n",
            "Epoch 13/50 - Training Loss: 0.0034 - Validation Loss: 0.0021\n",
            "Epoch 14/50 - Training Loss: 0.0038 - Validation Loss: 0.0021\n",
            "Epoch 15/50 - Training Loss: 0.0030 - Validation Loss: 0.0018\n",
            "Epoch 16/50 - Training Loss: 0.0032 - Validation Loss: 0.0040\n",
            "Epoch 17/50 - Training Loss: 0.0067 - Validation Loss: 0.0029\n",
            "Epoch 18/50 - Training Loss: 0.0050 - Validation Loss: 0.0018\n",
            "Epoch 19/50 - Training Loss: 0.0038 - Validation Loss: 0.0021\n",
            "Epoch 20/50 - Training Loss: 0.0050 - Validation Loss: 0.0019\n",
            "Epoch 21/50 - Training Loss: 0.0033 - Validation Loss: 0.0019\n",
            "Epoch 22/50 - Training Loss: 0.0027 - Validation Loss: 0.0014\n",
            "Epoch 23/50 - Training Loss: 0.0024 - Validation Loss: 0.0013\n",
            "Epoch 24/50 - Training Loss: 0.0028 - Validation Loss: 0.0017\n",
            "Epoch 25/50 - Training Loss: 0.0054 - Validation Loss: 0.0025\n",
            "Epoch 26/50 - Training Loss: 0.0032 - Validation Loss: 0.0016\n",
            "Epoch 27/50 - Training Loss: 0.0030 - Validation Loss: 0.0014\n",
            "Epoch 28/50 - Training Loss: 0.0028 - Validation Loss: 0.0016\n",
            "Epoch 29/50 - Training Loss: 0.0025 - Validation Loss: 0.0019\n",
            "Epoch 30/50 - Training Loss: 0.0029 - Validation Loss: 0.0012\n",
            "Epoch 31/50 - Training Loss: 0.0026 - Validation Loss: 0.0020\n",
            "Epoch 32/50 - Training Loss: 0.0025 - Validation Loss: 0.0013\n",
            "Epoch 33/50 - Training Loss: 0.0028 - Validation Loss: 0.0011\n",
            "Epoch 34/50 - Training Loss: 0.0027 - Validation Loss: 0.0012\n",
            "Epoch 35/50 - Training Loss: 0.0027 - Validation Loss: 0.0027\n",
            "Epoch 36/50 - Training Loss: 0.0059 - Validation Loss: 0.0014\n",
            "Epoch 37/50 - Training Loss: 0.0030 - Validation Loss: 0.0013\n",
            "Epoch 38/50 - Training Loss: 0.0027 - Validation Loss: 0.0012\n",
            "Epoch 39/50 - Training Loss: 0.0024 - Validation Loss: 0.0011\n",
            "Epoch 40/50 - Training Loss: 0.0024 - Validation Loss: 0.0011\n",
            "Epoch 41/50 - Training Loss: 0.0021 - Validation Loss: 0.0011\n",
            "Epoch 42/50 - Training Loss: 0.0020 - Validation Loss: 0.0012\n",
            "Epoch 43/50 - Training Loss: 0.0039 - Validation Loss: 0.0013\n",
            "Epoch 44/50 - Training Loss: 0.0028 - Validation Loss: 0.0016\n",
            "Epoch 45/50 - Training Loss: 0.0026 - Validation Loss: 0.0014\n",
            "Epoch 46/50 - Training Loss: 0.0026 - Validation Loss: 0.0015\n",
            "Epoch 47/50 - Training Loss: 0.0025 - Validation Loss: 0.0012\n",
            "Epoch 48/50 - Training Loss: 0.0025 - Validation Loss: 0.0011\n",
            "Epoch 49/50 - Training Loss: 0.0021 - Validation Loss: 0.0010\n",
            "Epoch 50/50 - Training Loss: 0.0021 - Validation Loss: 0.0010\n",
            "ğŸ“ˆ Score RÂ² sur le jeu de test : 0.9678\n",
            "âœ… Scaler sauvegardÃ© sous minmax_scaler.pkl\n"
          ]
        }
      ]
    }
  ]
}
