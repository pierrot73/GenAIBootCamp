{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUlmSVrHyax8+az9gb5ycd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierrot73/GenAIBootCamp/blob/Bootcamp/Week5_Day3_DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ckp1nlM4usga",
        "outputId": "5c4382c6-07fb-4dc7-9d55-077048d7a943"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 44) (ipython-input-1-2589111311.py, line 44)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-2589111311.py\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    Forme des images d'entraînement : (60000, 28, 28)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 44)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "Open In Colab\n",
        "Daily Challenge\n",
        "\n",
        "#Load the MNIST dataset:\n",
        "\n",
        "&\n",
        "\n",
        "Preprocess the data for a Fully Connected Neural Network:\n",
        "\n",
        "\n",
        "# Importation des bibliothèques nécessaires\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Chargement du jeu de données MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Affichage des formes des données pour comprendre leur structure\n",
        "print(\"Forme des images d'entraînement :\", X_train.shape)\n",
        "print(\"Forme des labels d'entraînement :\", y_train.shape)\n",
        "print(\"Forme des images de test :\", X_test.shape)\n",
        "print(\"Forme des labels de test :\", y_test.shape)\n",
        "\n",
        "# Prétraitement des images pour un réseau entièrement connecté\n",
        "\n",
        "# 1. Aplatir les images 28x28 en vecteurs de 784 pixels\n",
        "X_train = X_train.reshape((X_train.shape[0], 28 * 28))\n",
        "X_test = X_test.reshape((X_test.shape[0], 28 * 28))\n",
        "\n",
        "# 2. Normaliser les valeurs des pixels en divisant par 255\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# 3. Encoder les labels en one-hot\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Vérification des transformations\n",
        "print(\"Forme des images d'entraînement après prétraitement :\", X_train.shape)\n",
        "print(\"Forme des labels d'entraînement après encodage :\", y_train.shape)\n",
        "\n",
        "\n",
        "Forme des images d'entraînement : (60000, 28, 28)\n",
        "Forme des labels d'entraînement : (60000,)\n",
        "Forme des images de test : (10000, 28, 28)\n",
        "Forme des labels de test : (10000,)\n",
        "Forme des images d'entraînement après prétraitement : (60000, 784)\n",
        "Forme des labels d'entraînement après encodage : (60000, 10)\n",
        "Build and train a Fully Connected Neural Network:\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "# Flatten images and normalize pixel values\n",
        "x_train = x_train.reshape(-1, 28*28).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Create Sequential model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add input layer and first hidden layer\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(28*28,)))\n",
        "\n",
        "# Add more hidden layers if desired\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Output layer with softmax activation for multi-class classification\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "\n",
        "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
        "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
        "Epoch 1/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 3s 5ms/step - accuracy: 0.8113 - loss: 0.6622 - val_accuracy: 0.9503 - val_loss: 0.1745\n",
        "Epoch 2/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - accuracy: 0.9529 - loss: 0.1594 - val_accuracy: 0.9627 - val_loss: 0.1300\n",
        "Epoch 3/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9678 - loss: 0.1114 - val_accuracy: 0.9657 - val_loss: 0.1122\n",
        "Epoch 4/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9753 - loss: 0.0825 - val_accuracy: 0.9696 - val_loss: 0.1001\n",
        "Epoch 5/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 3s 5ms/step - accuracy: 0.9815 - loss: 0.0629 - val_accuracy: 0.9722 - val_loss: 0.0896\n",
        "Epoch 6/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9856 - loss: 0.0498 - val_accuracy: 0.9745 - val_loss: 0.0898\n",
        "Epoch 7/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 3s 5ms/step - accuracy: 0.9887 - loss: 0.0397 - val_accuracy: 0.9721 - val_loss: 0.0903\n",
        "Epoch 8/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 3s 6ms/step - accuracy: 0.9904 - loss: 0.0336 - val_accuracy: 0.9733 - val_loss: 0.0894\n",
        "Epoch 9/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.9920 - loss: 0.0278 - val_accuracy: 0.9732 - val_loss: 0.0937\n",
        "Epoch 10/10\n",
        "375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9930 - loss: 0.0241 - val_accuracy: 0.9740 - val_loss: 0.0989\n",
        "313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9749 - loss: 0.0971\n",
        "Test Loss: 0.0809\n",
        "Test Accuracy: 0.9776\n",
        "Preprocess the data for a Convolutional Neural Network:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Reshape the data to include the channel dimension\n",
        "# For grayscale images, the shape becomes (samples, height, width, channels)\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = len(np.unique(y_train))\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "Build and train a Convolutional Neural Network:\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import numpy as np\n",
        "import cv2  # pour le redimensionnement\n",
        "\n",
        "# Chargement du jeu de données MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Définir les dimensions d'entrée\n",
        "hauteur, largeur, canaux = 64, 64, 3\n",
        "\n",
        "# Préparer les images d'entraînement\n",
        "train_images = []\n",
        "\n",
        "for img in X_train:\n",
        "    # MNIST est en niveaux de gris, shape = (28, 28)\n",
        "    # Convertir en RGB en dupliquant le canal\n",
        "    img_rgb = np.stack([img]*3, axis=-1)\n",
        "    # Redimensionner à 64x64\n",
        "    img_resized = cv2.resize(img_rgb, (largeur, hauteur))\n",
        "    # Normaliser entre 0 et 1\n",
        "    img_normalized = img_resized.astype('float32') / 255.0\n",
        "    train_images.append(img_normalized)\n",
        "\n",
        "# Convertir en numpy array\n",
        "train_images = np.array(train_images)\n",
        "\n",
        "# Préparer les labels\n",
        "train_labels = y_train\n",
        "\n",
        "# Vérifier la forme\n",
        "print(\"Forme des images d'entraînement :\", train_images.shape)\n",
        "# Doit être (60000, 64, 64, 3)\n",
        "\n",
        "# Construction du modèle CNN\n",
        "model = Sequential()\n",
        "\n",
        "# Ajouter couches convolutionnelles\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(hauteur, largeur, canaux)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Aplatir pour les couches denses\n",
        "model.add(Flatten())\n",
        "\n",
        "# Ajouter couches denses\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Couche de sortie pour une classification multi-classe (10 classes)\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # adapté pour labels entiers\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Résumé du modèle\n",
        "model.summary()\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(train_images, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "\n",
        "Forme des images d'entraînement : (60000, 64, 64, 3)\n",
        "Model: \"sequential_13\"\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "│ conv2d_36 (Conv2D)              │ (None, 62, 62, 32)     │           896 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_36 (MaxPooling2D) │ (None, 31, 31, 32)     │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_37 (Conv2D)              │ (None, 29, 29, 64)     │        18,496 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_37 (MaxPooling2D) │ (None, 14, 14, 64)     │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_38 (Conv2D)              │ (None, 12, 12, 128)    │        73,856 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_38 (MaxPooling2D) │ (None, 6, 6, 128)      │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ flatten_12 (Flatten)            │ (None, 4608)           │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_39 (Dense)                │ (None, 128)            │       589,952 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_40 (Dense)                │ (None, 64)             │         8,256 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_41 (Dense)                │ (None, 10)             │           650 │\n",
        "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        " Total params: 692,106 (2.64 MB)\n",
        " Trainable params: 692,106 (2.64 MB)\n",
        " Non-trainable params: 0 (0.00 B)\n",
        "Epoch 1/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 292s 386ms/step - accuracy: 0.8846 - loss: 0.3615 - val_accuracy: 0.9791 - val_loss: 0.0683\n",
        "Epoch 2/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 287s 383ms/step - accuracy: 0.9870 - loss: 0.0426 - val_accuracy: 0.9887 - val_loss: 0.0400\n",
        "Epoch 3/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 319s 379ms/step - accuracy: 0.9924 - loss: 0.0254 - val_accuracy: 0.9899 - val_loss: 0.0364\n",
        "Epoch 4/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 322s 379ms/step - accuracy: 0.9934 - loss: 0.0194 - val_accuracy: 0.9854 - val_loss: 0.0494\n",
        "Epoch 5/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 323s 381ms/step - accuracy: 0.9937 - loss: 0.0178 - val_accuracy: 0.9893 - val_loss: 0.0397\n",
        "Epoch 6/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 322s 381ms/step - accuracy: 0.9959 - loss: 0.0136 - val_accuracy: 0.9897 - val_loss: 0.0407\n",
        "Epoch 7/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 285s 380ms/step - accuracy: 0.9961 - loss: 0.0115 - val_accuracy: 0.9889 - val_loss: 0.0459\n",
        "Epoch 8/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 285s 380ms/step - accuracy: 0.9968 - loss: 0.0095 - val_accuracy: 0.9885 - val_loss: 0.0393\n",
        "Epoch 9/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 322s 380ms/step - accuracy: 0.9974 - loss: 0.0080 - val_accuracy: 0.9900 - val_loss: 0.0479\n",
        "Epoch 10/10\n",
        "750/750 ━━━━━━━━━━━━━━━━━━━━ 322s 380ms/step - accuracy: 0.9972 - loss: 0.0089 - val_accuracy: 0.9899 - val_loss: 0.0439\n",
        "<keras.src.callbacks.history.History at 0x78cd05da4c90>\n",
        "Compare the performance:\n",
        "\n",
        "Analyse comparative des performances entre le modèle Fully Connected (Sequential) et le modèle Convolutional Neural Network (CNN) :\n",
        "\n",
        "1. Performance en termes de précision (accuracy)\n",
        "Modèle\tÉpoque 1\tÉpoque 10\tAccuracy finale (époque 10)\n",
        "Modèle Fully Connected\t81.13%\t99.30%\t99.30%\n",
        "CNN\t88.46%\t99.72%\t99.72%\n",
        "Le modèle CNN atteint une précision légèrement plus élevée que le modèle Fully Connected à la dernière époque.\n",
        "La différence de performance est notable dès la première époque, le CNN étant plus performant dès le début.\n",
        "2. Analyse de l'évolution de la performance\n",
        "Modèle Fully Connected :\n",
        "\n",
        "La précision commence à 81.13% et monte rapidement jusqu'à environ 99.3% en 10 epochs.\n",
        "La courbe d'apprentissage montre une croissance rapide, ce qui indique une bonne capacité à apprendre rapidement les caractéristiques du dataset.\n",
        "La validation suit presque parallèlement, atteignant environ 97.4%, ce qui montre un bon ajustement et peu de surapprentissage.\n",
        "Modèle CNN :\n",
        "\n",
        "La précision commence à un niveau plus élevé (88.46%) et atteint près de 99.72% à la fin.\n",
        "La progression est également rapide, mais avec des temps d'entraînement beaucoup plus longs.\n",
        "La validation atteint également un niveau élevé (~98.99%), avec une légère fluctuation mais globalement très bonne généralisation.\n",
        "3. Différences principales entre les deux modèles\n",
        "Aspect\tFully Connected (Sequential)\tCNN\n",
        "Architecture\tDense, toutes les entrées connectées\tConvolutional, exploitant la hiérarchie spatiale\n",
        "Capacité à capturer la structure spatiale\tFaible, car chaque pixel est traité individuellement\tForte, car convolution et pooling capturent les motifs locaux\n",
        "Temps d'entraînement\tRapide (en quelques secondes par époque)\tBeaucoup plus long (plusieurs minutes par époque)\n",
        "Performances\tTrès bonnes, mais légèrement inférieures\tLégèrement meilleures, surtout pour la généralisation\n",
        "4. Résumé de l’analyse\n",
        "Le CNN est clairement supérieur en termes de performance globale, principalement parce qu'il exploite la structure spatiale des images.\n",
        "La différence de précision est notable, surtout pour des tâches d’image où la spatialité est importante.\n",
        "En outre, le CNN tend à mieux généraliser, comme en témoigne la stabilité des valeurs de validation.\n",
        "Conclusion :\n",
        "Pour des tâches de classification d'images comme MNIST, les CNN sont généralement préférés car ils capturent mieux les motifs locaux.\n",
        "Même si le modèle Fully Connected donne de bons résultats, il est moins efficace en termes de capacité à comprendre la structure spatiale et nécessite souvent plus de paramètres.\n",
        "La différence de temps d'entraînement est significative, mais la performance supérieure du CNN justifie généralement le coût supplémentaire."
      ]
    }
  ]
}
